"""This file assembles components into final system. Includes testing functions to run experiments.
Includes "main" function (userInput) to interact with best-performing system versions by inputting nouns
individually."""

print("Start!")
from wordmodel import loadModel, getNearestNeighbours
import re
import fasttext
import matplotlib.pyplot as plt
import functools

#* Change default on load-up here
# classifier_model = fasttext.load_model("Data/models/sn_bronze_partialsen.bin")
classifier_model = fasttext.load_model("Data/models/n_gold_w.bin")
#* Change default testing file here
CANON_NOUNS = "Data/testing/canonNouns.txt"
#* Change model type here (facebook = variant; gensim = base)
WORDMODEL = loadModel(type="gensim")

# For counting number of NN types returned in a list
NN_Counts = {
    "NC": 0,
    "SC": 0,
    "OC": 0,
    "absPro": 0,
    "possC": 0,
    "adjPre": 0
}

# PREFIX DICTIONARIES--------------------
"""These were generated by putting the full and contracted prefixes through the phonological
conditioning functions"""
match_NCprefixes = {
    "umu": False,
    "um": False,
    "u": False, #["1a","11","14"],
    "mu": False,
    "aba":"2",
    "abe":"2",
    "ba": "2",
    "be": "2", 
    "o": "2a", # Make false bc can have o for 1 e.g. ofunday? Technicall
    "bo": "2a", # simple prefix form
    "imi": "4",
    "im": False, #"4", i (NC5) + m
    "mi": "4", # simple prefix2
    "ili": "5",
    "il": "5",
    "li": "5",
    "i": False,
    "ama": "6",
    "am": "6",
    "ma": "6", # simple prefix
    "isi": "7", 
    "is": False, #"7", i (NC5) + s, especially in cases of s-initiating loanwords, e.g. i-softhiwe, i-selula
    "si": "7", # simple prefix
    "izi": False,
    "iz": False,
    "zi": "8",
    "in": False, #"9", i (NC5) + n, e.g. ingovu
    "n": "9",
    "m" : "9", 
    "im": False, #"9", i (NC5) + m, e.g. imayonnaise
    "izim":False, #"14", izi (NC8) + consonant
    "izin":False, #"10", izi (NC8) + consonant
    "zin": "10", # simple
    "zim": "10", # simple
    "lu":"11",  # simple
    "ulu": "11",
    "ul": False, #"11", u (NC1a,3 etc) + consonant
    "ubu": False, #"14", u (NC1a,3 etc) + consonant
    "bu": "14", # simple
    "ub": False, #"14", u (NC1a,3 etc) + consonant
    "uku": "15",
    "ku": "15", # simple
    "uk": False, #"15", u (NC1a,3 etc) + consonant
    "pha": "16", # simple
    "ph": "16",  
}
match_NCprefixes = dict(sorted(match_NCprefixes.items(), key=lambda item: len(item[0]), reverse=True))
NC_prefixes = {'1': ['umu', 'umw', 'um', 'um', "mu","m", "mw"], '1a': ['u', 'w'], '2': ['aba', 'abe', 'abo', 'abe', 'ba', 'ab', 'bo', 'be', 'b'], '2a': ['o', "bo"], '3': ['umu', 'umw', 'um', 'u', 'w',"mu", "m"], '4': ['imi', 'imy', 'im', "mi", "my"], '5': ['i', 'y', "ili", "li"], '6': ['ame', 'ama', 'amo', 'ame', 'am'], '7': ['isi', 'isy', 'is'], '8': ['izi', 'izy', 'iz'], '9': ['im', 'in'], '10': ['izim', 'izin', 'izi', 'izi', 'izy', 'izy', 'iz', 'iz'], '11': ['ulu', 'ulw', 'ul', 'u', 'w'], '14': ['ubu', 'ubw', 'ub', 'u', 'w'], '15': ['uku', 'ukw', 'uk'], '17': ['uku', 'ukw', 'uk']}
SC_prefixes=  {'1': ['aka', 'ako', 'ake', 'ako', 'ake', 'ka', 'wa', 'ko', 'ke', 'wo', 'we', 'ak', 'ak', 'ko', 'ke', 'wo', 'we', 'u', 'w', 'k', 'w', 'k', 'w', 'w'], 
               '1a': ['ka', 'ko', 'ke', 'ko', 'ke', 'u', 'e', 'w', 'k', 'k', 'w'], 
               '2': ['aba', 'abo', 'abe', 'abo', 'abe', 'ba', 'be', 'bo', 'be', 'ab', 'ab', 'bo', 'be', 'b'], 
               '2a':['a', "ab", "aba"], '3': ['aka', 'awu', 'ako', 'ake', 'ako', 'ake', 'wa', 'ak', 'aw', 'wo', 'we', 'ak', 'aw', 'wo', 'we', 'u', 'w', 'w', 'w', 'w'], 
               '4': ['ayi', 'ayy', 'ayy', 'ya', 'ay', 'yo', 'ye', 'ay', 'yo', 'ye', 'i', 'y', 'y', 'y', 'y'], 
               '5': ['ali', 'aly', 'aly', 'li', 'la', 'ly', 'al', 'lo', 'le', 'al', 'ly', 'lo', 'le', 'l', 'l', 'l', 'l'], 
               '6': ['awa', 'awo', 'awe', 'awo', 'awe', 'aw', 'aw', 'a', 'o', 'e', 'o', 'e'], '7': ['asi', 'asy', 'asy', 'si', 'sa', 'sy', 'as', 'so', 'se', 'as', 'sy', 'so', 'se', 's', 's', 's', 's'], 
               '8': ['azi', 'azy', 'azy', 'zi', 'za', 'zy', 'az', 'zo', 'ze', 'az', 'zy', 'zo', 'ze', 'z', 'z', 'z', 'z'], '9': ['ayi', 'ayy', 'ayy', 'ya', 'ay', 'yo', 'ye', 'ay', 'yo', 'ye', 'i', 'y', 'y', 'y', 'y'], 
               '10': ['azi', 'azy', 'azy', 'zi', 'za', 'zy', 'az', 'zo', 'ze', 'az', 'zy', 'zo', 'ze', 'z', 'z', 'z', 'z'], 
               '11': ['alu', 'lwa', 'alw', 'lwo', 'lwe', 'alw', 'lwo', 'lwe', 'lu', 'lw', 'al', 'lw', 'al', 'lw', 'lw', 'l', 'l'], 
               '14': ['abu', 'aba', 'abw', 'abo', 'abe', 'abw', 'abo', 'abe', 'bu', 'ba', 'bw', 'bo', 'be', 'ab', 'ab', 'ab', 'ab', 'bw', 'bo', 'be', 'b', 'b', 'b', 'b'], 
               '15': ['aku', 'kwa', 'akw', 'kwo', 'kwe', 'akw', 'kwo', 'kwe', 'ku', 'kw', 'ak', 'kw', 'ak', 'kw', 'kw', 'k', 'k'], 
               '16': ['kwa', 'aku', 'kwo', 'kwe', 'akw', 'kwo', 'kwe', 'akw', 'ku', 'kw', 'kw', 'ak', 'kw', 'ak', 'kw', 'k', 'k'],
               '17': ['aku', 'kwa', 'akw', 'kwo', 'kwe', 'akw', 'kwo', 'kwe', 'ku', 'kw', 'ak', 'kw', 'ak', 'kw', 'kw', 'k', 'k']} 

OC_prefixes = {'1': ['mu', 'mw', 'm'], '1a': ['mu', 'mw', 'm'], '2': ['ba', 'bo', 'be', 'b'], '2a': ['ba', 'bo', 'be', 'b'], '3': ['wu', 'ww', 'w'], '4': ['yi', 'yy', 'y'], '5': ['li', 'ly', 'l'], '6': ['wa', 'wo', 'we', 'w'], '7': ['si', 'sy', 's'], '8': ['zi', 'zy', 'z'], '9': ['yi', 'yy', 'y'], '10': ['zi', 'zy', 'z'], '11': ['lu', 'lw', 'l'], '14': ['bu', 'bw', 'b'], '15': ['ku', 'kw', 'k'], '17': ['ku', 'kw', 'k']}
ABS_prefixes =  {'1': ['yena', 'yeno', 'yene', 'yen'], '1a': ['bona', 'bono', 'bone', 'bon'], '2': ['yena', 'yeno', 'yene', 'yen'], '2a': ['bona', 'bono', 'bone', 'bon'], '3': ['wona', 'wono', 'wone', 'won'], '4': ['yona', 'yono', 'yone', 'yon'], '5': ['lona', 'lono', 'lone', 'lon'], '6': ['wona', 'wono', 'wone', 'won'], '7': ['sona', 'sono', 'sone', 'son'], '8': ['zona', 'zono', 'zone', 'zon'], '9': ['yona', 'yono', 'yone', 'yon'], '10': ['zona', 'zono', 'zone', 'zon'], '11': ['lona', 'lono', 'lone', 'lon'], '14': ['bona', 'bono', 'bone', 'bon'], '15': ['khona', 'khono', 'khone', 'khon'], '17': ['khona', 'khono', 'khone', 'khon']}
POSS_prefixes = {'1': ['wa', 'ka', 'wo', 'we', 'ko', 'ke', 'w', 'k'], '1a': ['wa', 'ka', 'wo', 'we', 'ko', 'ke', 'w', 'k'], '2': ['baka', 'bako', 'bake', 'bak', 'ba', 'bo', 'be', 'b'], '2a': ['baka', 'bako', 'bake', 'bak', 'ba', 'bo', 'be', 'b'], '3': ['wa', 'ka', 'wo', 'we', 'ko', 'ke', 'w', 'k'], '4': ['ya', 'yo', 'ye', 'y'], '5': ['la', 'lo', 'le', 'l'], '6': ['a', 'o', 'e'], '7': ['sika', 'siko', 'sike', 'sik', 'sa', 'so', 'se', 's'], '8': ['zika', 'ziko', 'zike', 'zik', 'za', 'zo', 'ze', 'z'], '9': ['ya', 'yo', 'ye', 'y'], '10': ['zika', 'ziko', 'zike', 'zik', 'za', 'zo', 'ze', 'z'], '11': ['luka', 'luko', 'luke', 'lwa', 'lwo', 'lwe', 'luk', 'lw'], '14': ['buka', 'buko', 'buke', 'buk', 'ba', 'bo', 'be', 'b'], '15': ['kuka', 'kuko', 'kuke', 'kwa', 'kwo', 'kwe', 'kuk', 'kw'], '17': ['kuka', 'kuko', 'kuke', 'kwa', 'kwo', 'kwe', 'kuk', 'kw']}

# UTILITY FUNCTIONS -----------------------------------------

def get_subwords(word, min_n, max_n):
    """Generates word subwords in a range

    Args:
        word (String): _description_
        min_n (int):min length
        max_n (int): max length

    Returns:
        list: sorted list of subwords 
    """
    min_n = min_n -1
    subwords = set()  # Using a set to avoid duplicate subwords
    word_length = len(word)
    
    # Iterate over all possible subword lengths within the given range
    for length in range(min_n, max_n + 1):
        # Iterate over all possible positions in the word to extract subwords
        for i in range(word_length - length + 1):
            subword = word[i:i + length]
            subwords.add(subword)
        
        # Handle the case where the subword is the entire word
        if length <= word_length:
            subwords.add(word[:length])  # Prefix
            subwords.add(word[-length:]) # Suffix
    
    return list(sorted(subwords))
def checkSubwords(neighbor, prediction):
    """Predicts labels fro a nn's subwords and returns False if NOT INVALID
    Args:
        neighbour (String): nearest neighbour tuple
        prediction: predicted label
    Returns:
        list: sorted list of subwords 
    """
    global classifier_model
    
    
    subwords= get_subwords(neighbor[0], WORDMODEL.wv.min_n, WORDMODEL.wv.max_n)
    subwords_checker = set(classifier_model.get_subwords(neighbor[0])[0])
    
    for subword in subwords:
            label, _ = classifier_model.predict(subword.replace("<", "").replace(">", ""))
            if (subword != neighbor[0]) and (label[0]==prediction):
                return False
    return True

def isBadNeighbour(nearestNeighbour, POS, classNum, subwords=False):
    """Determines if a neighbour is syntactically removed or not
    Args:
        neighbour (String): nearest neighbour tuple  ( ("x",0.9) , NC1) )
        POS: predicted label
        classNum (int): as said
        subwords (Bool): Change to True to use Subword Syntactic Filter variant
    Returns:
        list: sorted list of subwords 
    """
    nearestNeighbour = nearestNeighbour[0]
    
    # Two possible interpretations of syntactic method
    #* Checks if has applicable morpheme for its given label
    
    if subwords:
        return checkSubwords(nearestNeighbour,f"__label__{POS}{classNum}" )
    else:
        # 15 and 17 have same POS sets
        if classNum=="15":
            classNum = "17"
        match POS:
            case("NC"):
                NN_Counts[POS] = NN_Counts[POS] +1
                for affix in NC_prefixes[classNum]:
                    if affix in nearestNeighbour[0]:
                        return False
            case("OC"):
                NN_Counts[POS] = NN_Counts[POS] +1
                for affix in OC_prefixes[classNum]:
                    if affix in nearestNeighbour[0]:
                        return False
            case("SC"):
                NN_Counts[POS] = NN_Counts[POS] +1
                for affix in SC_prefixes[classNum]:
                    if affix in nearestNeighbour[0]:
                        return False
            case("possC"):
                NN_Counts[POS] = NN_Counts[POS] +1
                for affix in POSS_prefixes[classNum]:
                    if affix in nearestNeighbour[0]:
                        return False
            case("absPro"):
                NN_Counts[POS] = NN_Counts[POS] +1
                for affix in ABS_prefixes[classNum]:
                    if affix in nearestNeighbour[0]:
                        return False
            case("adjPre"):
                return False
        return True

def extractTagAndClass(word):
    """Returns tag and class from label of form XXXNN
    Args:
        word (string): label or label and word with tag/class
    Returns:
        tag (String): pos/morpheme label
        class_part (String): class number
        OR
        None, None
        
    """
    # Use regular expression to separate the tag and class
    match = re.match(r"([a-zA-Z]+)(\d+[a|b]?)", word)
    if match:
        tag = match.group(1)
        class_part = match.group(2)
        return tag, class_part
    else:
        return None, None  # Return None if the word doesn't match the expected format

def getNNConcordPrediction(NN):
    """Predict concord label for a nearest neighbour subword
    Args:
        neighbour (String): nearest neighbour tuple  ( ("x",0.9) , NC1) )
    Returns:
        list: (neighbour, tag+class)
    """
    global classifier_model
    predict = classifier_model.predict(NN[0], k=1)
    return (NN, predict[0][0].split("__label__")[1])

def getGoodNeighbours(nearestNeighbours, syntacticToggledOn=True, useSubwords=False):
    """Filter out neighbours with concords that don't match the concord for the predicted class.
    Args:
        nearestNeighbours (tupe): ("NN", possXX)
    Returns:
        [("NN", XX)]: Neighbour and pos NUMBER -> . DOES NOT RETURN POSSXX, ONLY XX
    """
    
    neighbours = []
    for neighbour in nearestNeighbours:
        pos, num = extractTagAndClass(neighbour[1])
        # check if including syntactic disambiguation
        if syntacticToggledOn:
            
            if not isBadNeighbour(neighbour, pos, num, useSubwords):
                neighbours.append((neighbour[0],num))  
        else: # if syntactic toggled off, all neighbours are good
            neighbours.append((neighbour[0],num))  
    return neighbours
  
def getModalNNClass(nearestNeighbours):
    """Gets the NC with the highest frequency in the nearest neighbours list.
    Args:
        nearestNeighbours (list): List of NN and their predicted labels
    Returns:
        modal class (int): number of most frequenct NC (our NC prediction)
    """
    counter = {'1': 0, '1a':0, '2': 0,'2a':0, 
             '3': 0, '4': 0, '5': 0, '6': 0, 
             '7': 0, '8': 0, '9': 0, '10': 0, 
             '11': 0, '12': 0, '13': 0, '14': 0, 
             '15': 0, '16': 0, '17':0}
    if nearestNeighbours != []:
        for neighbour in nearestNeighbours:
            # incremement counter at NC index. 
            if str(neighbour[1]) in counter.keys():
                counter[str(neighbour[1])] += 1
            #ave_p += nearestNeighbours[0][1]
            
        return max(counter, key=counter.get)#, (ave_p/count)
    return 0

def getAve(a):
    
    if a[1][1] > 0:
        return (a[0], str(round(a[1][0]/a[1][1], 2)))
    return (str(a[0]),0)


# COMPONENT FUNCTIONS------------------------------------------
def prefixMethod(word):
    """Checks for a matching NC prefix. Does NOT detect non-Nouns.
    Args:
        word (string): Noun with a NC prefix to check
    Returns:
        False | int(NC_class): Returns False if AMBIGOUS or UNMATCHED prefix
    """
    word = word.lower()
    for prefix in match_NCprefixes:
        if word.startswith(prefix):
            return match_NCprefixes[prefix]
    return False

def removeVerbsfromNN(nn:list, variant = False):
    """Removes verbs from NN list using base (all verbs) or variant (mislassified SC verbs only) method.
    Args:
        nearestNeighbours (list): list of ("NN", possXX)
        variant (Bool): toggles vairant method on/off
    Returns:
        final_list (list): Final NN list of form [("NN", XX)]
    """
    global classifier_model
    sc_prefixes = ['b', 'lu', 'bu', 'zi', 'a', 'si', 'wu', 'z', 'i', 
                   'lw', 'ku', 'kw', 's', 'u', 'wa', 'be', 'l', 'ka', 
                   'y', 'ba', 'e', 'li', 'w', 'yi']

    final_list= []
    simple_classifier = fasttext.load_model("Data/models/raw_simplePOS_Classifier.bin")
    for n in  nn:
        prediction = simple_classifier.predict(n[0])
        if variant:
            if "V" in prediction[0][0]:
                #* Our regex checking for valid SC's
                patternSC =  r"""\b(se)?(aw|awu|mawu|ma|ka|kha|ma|a|be|b)?(a|be|nga|ng|b)?(lu|bu|zi|a|si|wu|z|i|lw|ku|kw|s|u|wa|be|l|ka|y|ba|e|li|w|yi)[a-zA-Z]+"""   # Must have verb stem at end
                compiled_pattern = re.compile(patternSC, re.VERBOSE) #verbose for multiline regexs
                if compiled_pattern.match(n[0]):
                    final_list.append(n)
        else:
            if "V" not in prediction[0][0]:
                final_list.append(n)
    return final_list

def semanticMethod(word, wordModel, NNType="classic", NNtopN=100, noVerbs=False, verbVariant=False):
    """Get a list of Nearest Neighbours for a query word. Removes verbs if Noverbs toggled on.
    Args:
        word (string): query noun
        wordModel (model object): word model 
        NNType (str, optional): annoy | classic NN. Defaults to "classic".
        NNtopN (int, optional):  Number of top N to get. Defaults to "100".
        noVerbs (False, optional): Toggle on to remove verbs
        verbVariant (False, optional): Toggle on to filter verbs based on variant method (no effect if noVerbs=False)
    Returns:
        nn (list): list off nn and their predictions
        semanticStrength | 0 (int): average similarity scores across set of nearest neighbours
    """
    global classifier_model

    nn =list(getNearestNeighbours(wordModel,word, NNtopN,NNType))
    if noVerbs:
        nn = removeVerbsfromNN(nn, verbVariant)
    if nn:
        semanticStrength_probability = round(functools.reduce(lambda acc, n: acc + n[1], nn,0),2)/len(nn)
        return nn, semanticStrength_probability
    else:
        print(f"Used nothing for {word}")
    return nn,0

def syntacticMethod(nearestNeighbours, toggledOn=True, subWordMethod=False):
    """Filters list of Nearest Neighbours for a query word based on whether they have a morpheme that matches their
    predicted label or not. 
    Args:
        nearestNeighbours (list): list of NN
        toggledOn (Bool, optional): Syntactic method is used by default. Set to False to omit from system.
        subwordMethod (Bool, optional): Checks syntax at word-level by default, Toggle on to check at subword level.

    Returns:
        mode (int): final predicted NN class for query word.
    """
    # nnPredictions: ("neighbour"", POSxx) -> Actually a map object iterable
    nnPredictions = list(map(getNNConcordPrediction, nearestNeighbours))
    
    # If syntactic Method toggled off, all neighbours are true neighbours
    true_neighbours = getGoodNeighbours(nnPredictions, syntacticToggledOn=toggledOn, useSubwords=subWordMethod)
    #print("Filtered neighbours: ", true_neighbours)
    mode = getModalNNClass(true_neighbours)
    return mode

def getNounNC(queryNoun, wordModel, NNType, topN, syntacticOn, 
              no_v=False, useVerbVariant = False,subwords=False):
    """Main function that gets an NC for a single noun.

    Args:
        queryNoun (String): query noun
        wordModel (model): word model to use
        NNType ("annoy"|"classic"): Always use classic! Annoy was removed.
        topN (int): K for KNN to use.
        syntacticOn (Bool): Toggle to True to use syntactic method
        no_v (Bool, optional): Toggle to True to remove verbs. Defaults to False.
        useVerbVariant (bool, optional): Toggle to True to use vairant verb method. Defaults to False.
        subwords (bool, optional): Toggle to True to use subword-level syntactic checking (not recommended). Defaults to False.

    Returns:
        final_prediction (String): NC + int
        probability_strength (int): Ignore. Returns average similarity across NN used.
    """    
    global classifier_model, classifier_model_path
    
    morph =prefixMethod(queryNoun)
    if morph:
        return morph, 1
    neighbours, probablityStrength = semanticMethod(queryNoun, wordModel,NNType, topN, no_v, verbVariant = useVerbVariant)
    if (neighbours == []) and no_v:
        print("Empty neighbours for", queryNoun)
        prediction = classifier_model.predict(queryNoun)
        _, prediction = extractTagAndClass(prediction[0][0].split("__label__")[1])

        return prediction, 1
    final_prediction = syntacticMethod(neighbours, toggledOn=syntacticOn, subWordMethod=subwords)
    return final_prediction, probablityStrength


# TESTING FUNCTIONS----------------------------------------------
def testClassifierMethod( testSet=CANON_NOUNS):
    """Tests a particular classifier's accuracy only against large noun set by default.
    Prints accuracy.
    Args:
        testSet (string, optional): Defaults to CANON_NOUNS.
    """     """"""
    
    global classifier_model
    # print("Testing system against given test set.")
    count_dict = {"NC"+nc:(0,0) for nc in ["1","1a","2","2a","3","4","5","6","7","8","9","10","11","14","15"]}
    correct = 0
    total = 0
    num_tests = 0
    
    with open(testSet, "r") as testFile:
        for line in testFile:
            #print(line.split(" ", maxsplit=1))
            label, noun = line.split(" ", maxsplit=1)
            prediction = classifier_model.predict(noun.strip().lower())
            label = extractTagAndClass(label.split("__label__")[1])
            num_tests +=1
            
            #print(label, prediction)
            i = "NC"+str(label[1])
            _, prediction = extractTagAndClass(prediction[0][0].split("__label__")[1])
            
            #print(label[1], prediction)
            if label[1] == prediction:
                # Tuple of format: (correct, total)
                # If correct, incremement BOTH correct AND total
                count_dict[i] = ((count_dict[i][0]+1),(count_dict[i][1]+1))
                correct+=1
            else:
             # If incorrect, incremement ONLY total
                count_dict[i] = ((count_dict[i][0]),(count_dict[i][1]+1))
            total +=1
    accuracy = correct/total
    print(f"Overall Accuracy: {accuracy}")
    count_dict = dict(map((getAve), count_dict.items()))
    print(f"Accuracy by class for {classifier_model} ")
    for nc, count in count_dict.items():
        print(f"{nc}: {count}")
    count_dict = list(count_dict.values())
    return accuracy, count_dict

def testPrefixMethod(testSet = CANON_NOUNS):
    print("Testing system against given test set.")
    count_dict = {"NC"+nc:(0,0) for nc in ["1","1a","2","2a","3","4","5","6","7","8","9","10","11","14","15"]}
    correct = 0
    total = 0
    # Probability score is an average of all the probability strengths
    probability_score = 0
    with open(testSet, "r") as testFile:
        for line in testFile:
            label, noun = line.split(" ", maxsplit=1)
            prediction = prefixMethod(noun.strip().lower())
            label = extractTagAndClass(label.split("__label__")[1])
            i = "NC"+str(label[1])
            if prediction:
                if str(label[1]) == prediction:
                    count_dict[i] = ((count_dict[i][0]+1),(count_dict[i][1]+1))
                    correct+=1
             
                count_dict[i] = ((count_dict[i][0]),(count_dict[i][1]+1))
            total +=1
    accuracy = correct/total
    return accuracy

def testSystem(topN_NN,useSyntactic, testSet = CANON_NOUNS,returnClassAccuracies=False, 
               noVerbs =False,verbVariant = False, useSubwords = False):
    global classifier_model
    print("Testing system against given test set.")
    count_dict = {"NC"+nc:(0,0) for nc in ["1","1a","2","2a","3","4","5","6","7","8","9","10","11","14","15"]}
    correct = 0
    total = 0
    num_tests = 0
    # Probability score is an average of all the probability strengths
    probability_score = 0
    with open(testSet, "r") as testFile:
        for line in testFile:
            label, noun = line.split(" ", maxsplit=1)
            prediction, probability_strength = getNounNC(noun.strip().lower(), WORDMODEL, 
                                                         "classic", topN_NN, syntacticOn =useSyntactic,
                                                         no_v= noVerbs, useVerbVariant = verbVariant, subwords=useSubwords)
            label = extractTagAndClass(label.split("__label__")[1])
            probability_score += probability_strength
            num_tests +=1
            i = "NC"+str(label[1])
            if str(label[1]) == prediction:
                # Tuple of format: (correct, total)
                # If correct, incremement BOTH correct AND total
                count_dict[i] = ((count_dict[i][0]+1),(count_dict[i][1]+1))
                correct+=1
            else:
                count_dict[i] = ((count_dict[i][0]),(count_dict[i][1]+1))
            total +=1
    accuracy = correct/total
    probability_score= probability_score/num_tests
    count_dict = (dict(map(getAve, count_dict.items())))
    
    if returnClassAccuracies:
        return accuracy, list(count_dict.values())
    print(accuracy)
    return accuracy, probability_score

def getAllClassifierAccuracy_OneKNN(quantity):
    """Gets accuracy for all 6 classifiers for 1 NN quantity

    Args:
        quantity (int): amount of NN to use

    Returns:
        accuracies [list]: List with each classifier's accuracy for given NN amount
    """
    global classifier_model
    models = ["Data/models/sn_bronze_fullsen.bin", # full sen: NC + SC
              "Data/models/sn_bronze_partialsen.bin", # partial sen: NC + SC
              "Data/models/sn_bronze_w.bin", # words: NC + SC
              "Data/models/n_bronze_w.bin", # words: only NC
              "Data/models/full_gold_w.bin", # words: full tagset -> ann
              "Data/models/sn_gold_w.bin",# words: nc + sc -> ann
              "Data/models/gold_nn_w.bin"
    ]

    accuracies = []
    for m in models:
        print(f"Model: {m}, KNN: {quantity}")
        classifier_model = fasttext.load_model(m)
        totAccuracy, _= testSystem(topN_NN=quantity, useSyntactic=True)
        accuracies.append(round(totAccuracy*100, 2))
    return accuracies

# GRAPHING AND EXPERIMENTS------------------------------------------------------
    
def plotNNAccuracies(figName):
    print("Graphing NN accuracies...")
    topN = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]
    acc = [[],[],[],[],[],[],[]]
    #probabilites = []
    
    for quantity in topN:
        print(f"Getting accuracy for {quantity}")
        c_accuracy= getAllClassifierAccuracy_OneKNN(quantity)   
        for i in range(7):
            acc[i].append(c_accuracy[i])
    
    col = ["darkgreen",  "lime","orange","darkorchid","red","blue", "violet"]
    labels = ['SC+N-Bronze-FullSen', #x
              'SC+N-Bronze-PartialSen', #x
              'SC+NC,Bronze,Words', #v
               'N-Bronze-Words', #o
               'Full-Gold-Words', #*
               'SC+N-Gold-Words', #o
               'N-Gold-Words']
    lines = ["solid","solid","solid","solid","dashed","dashed", "dashed"]
    markers = ["x","x", "v", "o","*","v","o" ]
    
    for i in range(7):
        print(f"Plotting {topN} nn for {labels[i]}")
        plt.plot(topN, acc[i], label=labels[i], color = col[i], linestyle = lines[i], marker = markers[i])
     
    plt.xlabel('topK NN used')
    plt.ylabel('Accuracy by Classifier')
    plt.title('Change in performance over KNN amount, by classifier type')
    plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1))
    print("Done!")
    plt.savefig(f"Data/results/{figName}.png")
    plt.show()
    plt.close()
    print("Graph closed.")

def recordBestWorstClassAccuracy():
    result = "Data/results/best_worst_classAccuracy_byKNN.csv"
    print(f"Writing best and worst class accuracies to {result}")

    labels = ['Auto: SC + NC, Full sentence', #x
        'Auto: SC + NC, N + V sentence', #x
        'Auto: SC + NC, words', #v
        'Auto: NC, words', #o
        'Annot.: full set, words', #*
        'Annot.: SC + NC, words'] #v
    # hard coded, recorded from plotNNAccuracies() experiment
    
    best = [20, 180, 140, 25, 170, 100]
    worst = [10, 20, 140, 10, 50, 150]
    
    global classifier_model
    models = ["Data/models/raw_full_sentence.bin", # full sen: NC + SC
              "Data/models/raw_sentence.bin", # partial sen: NC + SC
              "Data/models/test_raw_words.bin", # words: NC + SC
              "Data/models/raw_noun_words.bin", # words: only NC
              "Data/models/best_classifier.bin", # words: full tagset -> ann
              "Data/models/nc_sc_data_classifier.bin" # words: nc + sc -> ann
    ]
    with open(result, "w") as outfile: 
        # For each model, use best and worst and write class accuracies on line
        # line1: best
        # line2: worst
        
        # first write headers
        outfile.write("classifier;NNType;KNN;1;1a;2;2a;3;4;5;6;7;8;9;10;11;14;15")
        # then start writing accuracies
        lineContent = ""
        
        for i in range(6):
            classifier_model = fasttext.load_model(models[i])
            # line 1: best
            _, byClassAcc= testSystem(topN_NN=best[i], useSyntactic=True, returnClassAccuracies=True)
            lineContent = labels[i] +";best;" + str(best[i])+(";".join(map(str,byClassAcc[:-1])))
            outfile.write(lineContent+"\n") # Write best line to file
            
            # line 2: worst
            _, byClassAcc= testSystem(topN_NN=worst[i], useSyntactic=True, returnClassAccuracies=True)
            lineContent = labels[i] +";worst;"+str(worst[i])+(";".join(map(str,byClassAcc[:-1])))
            outfile.write(lineContent+"\n") # Write best line to file
            lineContent = ""
        print(f"Completed for {models[i]}")
    print("All done!")
            
def recordAllAccuracies():
    global classifier_model
    
    result = "Data/results/allAccuracies_alexVer.csv"
    #classResult = "Data/results/allAccuracies_byClass.csv"
    print(f"Writing all accuracies to {result}")
    
#* We are testing the top two best performing classifiers
#* And showing the accuracy at each component step.
    testSets = ["Data/testing/alexNouns.txt", "Data/testing/canonNouns_Nicky.txt"]
    systemVersion = [("M0", "Morph Only"),  
                     ("A1","Classifier only"), 
                     ("A2", "...with Semantic"),
                     #("A3","...with No Verbs" ),
                     ("A4","... with Syn., NO Verbs"),
                     ("A5", "...with Syn., AND Verbs")]
   
    classifiers = [('SN-Bronze-FullSen',"Data/models/sn_bronze_fullsen.bin", 20), #x
        ('SN-Bronze-PartialSen',"Data/models/sn_bronze_partialsen.bin",180), #x
        ('SN-Bronze-Words',"Data/models/sn_bronze_w.bin", 200), #v
        ('N-Bronze-Words',"Data/models/n_bronze_words.bin",20) ,#o
        ('Full-Gold-Words', "Data/models/full_gold_words.bin", 60), #*
        ('SN-Gold-Words',"Data/models/sn_gold_w.bin", 20),
        ('N-Gold-Words',"Data/models/n_gold_w.bin", 30)]#v
    
    with open(result, "a") as outfile: #, open(classResult, "a") as class_out :
            outfile.write("Code;Version;ClassifierName;Total\n")
            # print(f"{code};{version};{name};{ str(t0) };{ str(t1) }\n")
            for code, version in systemVersion:
                match(code):
                    case "0":
                            mAccT1 = testPrefixMethod(testSet=testSets[0])
                            # mAccT2 = testPrefixMethod(testSet=testSets[1])
                            #
                            outfile.write(f"{code};{version};morphOnly;{str(mAccT1)}\n")
                            print(f"{code};{version};morphOnly{ str(mAccT1)}\n")                                                   

                    case "A1":
                        # Classifier only
                            for name, modelPath, _ in classifiers:
                                print(f"Doing {version} for {name}")
                                classifier_model = fasttext.load_model(modelPath)
                                t0, t0Class =testClassifierMethod(testSets[0]) #classifier_model.test(testSets[0])
                                # t1, t1Class =testClassifierMethod(testSets[1]) #classifier_model.test(testSets[1])
                                print(f"{code};{version};{name};{ str(t0) };\n")

                                outfile.write(f"{code};{version};{name};{ str(t0) }\n")
                                #class_out.write(f"{code};{version};{name};Set1;{';'.join(t0Class)}\n")
                                #class_out.write(f"{code};{version};{name};Set2;{';'.join(t1Class)}\n")
                    case "2":
                        # With semantic 
                            for name, modelPath, knn in classifiers:
                                print(f"Doing {version} for {name}")
                                classifier_model = fasttext.load_model(modelPath)
                                t0,t0Class = testSystem(topN_NN=knn, useSyntactic=False, testSet=testSets[0], returnClassAccuracies=True)
                                print(f"{code};{version};{name};{ str(t0) };\n")

                                outfile.write(f"{code};{version};{name};{ str(t0) }\n")
                                #class_out.write(f"{code};{version};{name};Set1;{';'.join(t0Class)}\n")
                                #class_out.write(f"{code};{version};{name};Set2;{';'.join(t1Class)}\n")
                    case "3":
                        # Semantic + no verbs
                            for name, modelPath, knn in classifiers:
                                print(f"Doing {version} for {name}")
                                classifier_model = fasttext.load_model(modelPath)
                                t0,t0Class = testSystem(topN_NN=knn, useSyntactic=False, testSet=testSets[0],
                                                        noVerbs=True, returnClassAccuracies=True)
                          
                                outfile.write(f"{code};{version};{name};{ str(t0) };\n")
                                print(f"{code};{version};{name};{ str(t0) };\n")

                                #class_out.write(f"{code};{version};{name};Set1;{';'.join(t0Class)}\n")
                                #class_out.write(f"{code};{version};{name};Set2;{';'.join(t1Class)}\n")
                    case "4":
                        # Semantic + no verbs + syntactic
                            for name, modelPath, knn in classifiers:
                                print(f"Doing {version} for {name}")
                                classifier_model = fasttext.load_model(modelPath)
                                t0,t0Class = testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[0],
                                                        noVerbs=True, returnClassAccuracies=True, useSubwords=False)
                                print(f"{code};{version};{name};{ str(t0) }\n")
                                outfile.write(f"{code};{version};{name};{ str(t0) };\n")
                                #class_out.write(f"{code};{version};{name};Set1;{';'.join(t0Class)}\n")
                                #class_out.write(f"{code};{version};{name};Set2;{';'.join(t1Class)}\n")
                    case "5":
                        # Semantic + VERBS + syntactic
                            for name, modelPath, knn in classifiers:
                                print(f"Doing {version} for {name}")
                                classifier_model = fasttext.load_model(modelPath)
                                t0,t0Class = testSystem(topN_NN=knn, useSyntactic=True,testSet=testSets[0], 
                                                        returnClassAccuracies=True, useSubwords=False) # NoV - False
                                print(f"{code};{version};{name};{ str(t0) }\n")
                                outfile.write(f"{code};{version};{name};{ str(t0) };\n")
                                # class_out.write(f"{code};{version};{name};Set1;{';'.join(t0Class)}\n")
                                # class_out.write(f"{code};{version};{name};Set2;{';'.join(t1Class)}\n")

    print(f"Done. Results written to {result}")
def get_highest_misprediction_percentage(data):
    # Get the total value
    total = data.get("Total")  # Use 1 to avoid division by zero if 'total' is not found
    total_error = round(data.get("Total Error")/total,2)
    # Remove the 'total' key-value pair
    data_without_total = {k: v for k, v in data.items() if (k != "Total" and k != "Total Error")}
    
    # get hgihest
    sorted_items = sorted(data_without_total.items(), key=lambda x: x[1], reverse=True)
    highest_items = sorted_items[:2]
    
    # Calculate the percentage of each highest value relative to the total
    # but first add the total error
    highest_with_percentage = [("Total Error:", total_error)]
    highest_with_percentage.extend([(key, round((value / total) * 100,2)) for key, value in highest_items])
    
    return highest_with_percentage
    # Find the key-value pair with the highest value
    # highest_key = max(data_without_total, key=data_without_total.get)
    # highest_value = data_without_total[highest_key]

    # # Calculate the percentage of the highest value relative to the total
    # percentage = round((highest_value / total) * 100,2)
    
    # return (highest_, percentage)

def recordVerbFilterVariant():
    # syntactic, large model, etc + NO VERBS
    # 
    global classifier_model
    testSets = ["Data/testing/canonNouns.txt", "Data/testing/canonNouns_Nicky.txt"]
    result = "Data/results/allAccuracies.csv"

    classifiers = [('SN-Bronze-FullSen',"Data/models/raw_full_sentence.bin", 20), #x
        ('SN-Bronze-PartialSen',"Data/models/raw_sentence.bin",180), #x
        ('SN-Bronze-Words',"Data/models/test_raw_words.bin", 200), #v
        ('N-Bronze-Words',"Data/models/raw_noun_words.bin",20) ,#o
        ('Full-Gold-Words', "Data/models/best_classifier.bin", 60), #*
        ('SN-Gold-Words',"Data/models/nc_sc_data_classifier.bin", 20),
        ('N-Gold-Words',"Data/models/gold_nn_w.bin", 30)]#v
    with open(result, "a") as outfile:
        for name, modelPath, knn in classifiers:
            classifier_model = fasttext.load_model(modelPath)
            t0,_ = testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[0],noVerbs=True, verbVariant = True,returnClassAccuracies=False)
            t1,_= testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[1],noVerbs=True,verbVariant = True, returnClassAccuracies=False)
            outfile.write(f"Verb Filter Variant;VFilter;{name};{ str(t0) };{str(t1) }\n")
            
def recordSmallModelVariant():
        # syntactic, large model, etc + NO VERBS
    # 
    global classifier_model
    testSets = ["Data/testing/canonNouns.txt", "Data/testing/canonNouns_Nicky.txt"]
    result = "Data/results/allAccuracies.csv"
    classifiers = [('SN-Bronze-FullSen',"Data/models/raw_full_sentence.bin", 20), #x
        ('SN-Bronze-PartialSen',"Data/models/raw_sentence.bin",180), #x
        ('SN-Bronze-Words',"Data/models/test_raw_words.bin", 200), #v
        ('N-Bronze-Words',"Data/models/raw_noun_words.bin",20) ,#o
        ('Full-Gold-Words', "Data/models/best_classifier.bin", 60), #*
        ('SN-Gold-Words',"Data/models/nc_sc_data_classifier.bin", 20),
        ('N-Gold-Words',"Data/models/gold_nn_w.bin", 30)]#v
    # classifiers = [('SN-Bronze-FullSen',"Data/models/raw_full_sentence.bin", 20), #x
    #     ('SN-Bronze-PartialSen',"Data/models/raw_sentence.bin",180), #x
    #     ('SN-Bronze-Words',"Data/models/test_raw_words.bin", 140), #v
    #     ('N-Bronze-Words',"Data/models/raw_noun_words.bin",25) ,#o
    #     ('Full-Gold-Words', "Data/models/best_classifier.bin", 170), #*
    #     ('SN-Gold-Words',"Data/models/nc_sc_data_classifier.bin", 100),
    #     ('N-Gold-Words',"Data/models/gold_nn_w.bin", 100)]#v
    with open(result, "a") as outfile:
        for name, modelPath, knn in classifiers:
            classifier_model = fasttext.load_model(modelPath)
            t0,_ = testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[0],noVerbs=True, verbVariant = False,returnClassAccuracies=False)
            t1,_= testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[1],noVerbs=True, verbVariant=False,returnClassAccuracies=False)
            outfile.write(f"Small Model Variant;SmallModel;{name};{ str(t0) };{ str(t1) }\n")

def recordSyntacticVariant():
        # syntactic, large model, etc + NO VERBS
    global classifier_model
    testSets = ["Data/testing/canonNouns.txt", "Data/testing/canonNouns_Nicky.txt"]
    result = "Data/results/allAccuracies.csv"

    classifiers = [('SN-Bronze-FullSen',"Data/models/raw_full_sentence.bin", 20), #x
        ('SN-Bronze-PartialSen',"Data/models/raw_sentence.bin",180), #x
        ('SN-Bronze-Words',"Data/models/test_raw_words.bin", 200), #v
        ('N-Bronze-Words',"Data/models/raw_noun_words.bin",20) ,#o
        ('Full-Gold-Words', "Data/models/best_classifier.bin", 60), #*
        ('SN-Gold-Words',"Data/models/nc_sc_data_classifier.bin", 20),
        ('N-Gold-Words',"Data/models/gold_nn_w.bin", 30)]#v
    with open(result, "a") as outfile:
        for name, modelPath, knn in classifiers:
            classifier_model = fasttext.load_model(modelPath)
            t0,_ = testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[0],noVerbs=True,verbVariant = True,returnClassAccuracies=False,
                            useSubwords=True)
            t1,_= testSystem(topN_NN=knn, useSyntactic=True, testSet=testSets[1],noVerbs=True,verbVariant = True, returnClassAccuracies=False,
                             useSubwords=True)
            outfile.write(f"Syntactic Filter Variant;SynFilter;{name};{ str(t0) };{str(t1) }\n")

def recordVersionInaccuracies(version, NN, noVerbs):
    testSet = "Data/testing/canonNouns.txt"
    global classifier_model
    classifier_model = fasttext.load_model(version)
    
    count_dict = {nc:0 for nc in ["1","1a","2","2a","3","4","5","6","7","8","9","10","11","14","15"]}
    for nc in list(count_dict.keys()):
        count_dict[nc] = {errorClass:0 for errorClass in ["Total","Total Error","1","1a","2","2a","3","4","5","6","7","8","9","10","11","14","15"]}
    # Probability score is an average of all the probability strengths
    with open(testSet, "r") as testFile:
        for line in testFile:
            label, noun = line.split(" ", maxsplit=1)
            prediction, _= getNounNC(noun.strip().lower(), WORDMODEL, 
                                      "classic", NN, syntacticOn =True,
                                       no_v= noVerbs, useVerbVariant = False, subwords=False)
            label = extractTagAndClass(label.split("__label__")[1])
        
            actualClass = str(label[1])
            if actualClass == prediction:
                count_dict[actualClass]["Total"] = count_dict[actualClass]["Total"]+1
            else:
                # increment total and inaccuracies
                count_dict[actualClass]["Total"] = count_dict[actualClass]["Total"]+1
                count_dict[actualClass]["Total Error"] = count_dict[actualClass]["Total Error"]+1
                count_dict[actualClass][prediction] = count_dict[actualClass][prediction]+1
            
    for nc in list(count_dict.keys()):
        count_dict[nc] = get_highest_misprediction_percentage(count_dict[nc])
    with open("Data/results/errorByClass.csv", "a") as outfile:
        outfile.write(f"version;nc;total_error;maxSource;maxError;secondSource;secondError\n")
        for nc in count_dict.items():
            if nc[1][0][1]== 0.0:
                outfile.write(f"{version};{nc[0]};0;na;0;na;0\n")
                print(f"{nc[0]} errors: none")
            else:
                outfile.write(f"{version};{nc[0]};{nc[1][0][1]};{nc[1][1][0]};{nc[1][1][1]};{nc[1][2][0]};{nc[1][2][1]}\n")
                print(f"{nc[0]} errors: {nc[1]}")
    return count_dict

# DEMO / USER INTERFACE ----------------------------------------------------------

def userInterface():
    global classifier_model
    """Allows the user to use one of the two best classifier versions to
    get a noun's class for a singular noun.
    """
    nn= 180
    verbs=False
    print("WELCOME\nUsing base components and sn-bronze-partialsen classifier.\n")
    while True:
        global classifier_model
        menu = "Enter a noun, c to change between top two best performing classifiers, q to end.\n"
        option = input(menu)
        match option:
            
            case("c"):
                while True:
                    classifier_option = input("CLASSIFIERS:\n(1) sn-bronze-partialSen\n(2) nn-gold-words\n")
                    if classifier_option == "1":
                        classifier_model=fasttext.load_model("Data/models/sn_bronze_partialsen.bin")
                        nn= 180
                        break
                    elif classifier_option == "2":
                        classifier_model=fasttext.load_model("Data/models/gold_nn_w.bin")
                        nn = 30
                        break
                    else:
                        print("Try again. Option not recognised. Input 1 or 2.")
            case ("q"):
                print("Closing.")
                break
            case _:
                prediction, _ = getNounNC(option.strip().lower(),WORDMODEL,NNType="classic", topN=nn,syntacticOn=True,no_v=False)
                print(f"Noun class for {option}: NC{prediction}")
                
if __name__ == "__main__":
    #testClassifierMethod()
    #testSystem(topN_NN=60, useSyntactic=True)
    recordAllAccuracies()
    # print(f"Doing {version} for {name}")
    # classifier = ('Full-Gold-Words', "Data/models/best_classifier.bin", 60), #*
    # code = "test"
    # version = "...with semantic"
    # knn = 60
    # modelPath  = "Data/models/full_gold_words.bin"
    # name = 'Full-Gold-Words'
    # classifier_model = fasttext.load_model(modelPath)
    # t0,t0Class = testSystem(topN_NN=knn, useSyntactic=False, testSet=CANON_NOUNS, returnClassAccuracies=True)
    # print(f"{code};{version};{name};{ str(t0) };\n")

